{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Data Science in Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lab will introduce you to some basic tools and libraries commonly used for Data Science in python.\n",
    "<ul>\n",
    "<li>```numpy``` is a library that efficiently stores matrices and vectors, called arrays. Some really clever people optimised this very well, so that it can do things such as dot products and data compression very efficiently.\n",
    "<li>```pandas``` offers some more R-like dataframes. A dataframe is more flexible than an array (for example, it can have strings and numbers in the same column).\n",
    "<li>```matplotlib``` is a plotting library.\n",
    "<li>```seaborn``` is built on top of matplotlib, and offers a few additional perks in the same style as ```matplotlib```.\n",
    "</ul>\n",
    "\n",
    "## 1. Jupyter\n",
    "Jupyter notebooks are interactive python notebooks, in which you can design your code and run each cell one by one. There are code cells and markdown cells. You can select the type of cell above: Cell > Cell Type. Use markdown cells to write notes for your future self or your collaborators/readers. Code cells run your code when you hit ```Ctrl + Enter```. In <b>command mode</b>, you can navigate through cells using the up/down arrows. With ```Enter```/double click, you get into <b>execute mode</b>, where you can modify and execute cells.\n",
    "<ul>\n",
    "<li>Create a new cell in command mode by hitting ```a``` (new cell above) or ```b``` (new cell below).\n",
    "\n",
    "<li>Delete a cell in command mode by hitting ```d d```.\n",
    "</ul>\n",
    "A very neat feature of jupyter notebooks is that the output of the last command is automatically printed below a code cell, even without the print() function.\n",
    "\n",
    "<ol>\n",
    "<li>Try creating a new cell below (by default, this creates a new code cell), and put ```34 * 567``` in there. Execute the cell to see what happens. \n",
    "\n",
    "<li>Then, put ```34 / 567``` in there, below your previous line of code. What gets displayed? Two numbers, or just one? Which one?\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "34*567\n",
    "34/567"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's get started. First, import these libraries. This is the standard way in which many programs import them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Numpy\n",
    "\n",
    "Numpy is a library for storing and fast computing with matrices and vectors. The basic data type in numpy is a nupy array (```np.array()```). An array can be unidimensional (vector), two dimensional (matrix) or higher dimensional (\"tensor\"; like a matrix of matrices, etc. - but don't worry about it too much for now).\n",
    "\n",
    "A one dimensional array is a bit like a list in python, except that it has a fixed size that has to be defined when the array is created. It also has a fixed datatype, e.g. ```np.int64``` (integers) or ```np.float```. All elements of the array must be of the same datatype, and tying to insert an item of a different data type will create errors. Here are a few ways of creating new arrays. Try to execute the cells below and figure out how these methods work.\n",
    "\n",
    "You can also find more documentation on numpy <a href=\"http://www.numpy.org/\">here</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.array([1,2,3,9,8,7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.arange(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With ```np.array()```, you can turn a list or iterable into a numpy array. You need to specify the items by hand. ```np.arange``` automatically produces values within a specified range. Try creating an array that has all the odd number between 10 and 20. Check out <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.arange.html\">the documentation</a> to help you. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now run the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.zeros((5,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.empty((5,))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What's the difference between ```np.zeros``` and ```np.empty```? (You might not see any difference at first. If not, try executing the last cell again, until you see any difference.) What kinds of numbers are inside the \"empty\" array? Why is that? Check out <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.empty.html\">the documentation</a> for help.\n",
    "\n",
    "Here's yet another function that creates a new array - this can be quite handy for certain types of machine learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.full((5,),fill_value=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.rand(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Run the above cell multiple times, to see which numbers it produces and that they are indeed random floats between 0 and 1.)\n",
    "\n",
    "```np.zeros()```, ```np.empty()```, ```np.full()``` and ```np.random.rand()``` take in a \"shape\" as argument. Valid shapes are \n",
    "\n",
    "(x,)  -- a vector with length x\n",
    "(x,y) -- a x by y matrix\n",
    "(x,y,z) -- a x by y by z tensor\n",
    "etc.\n",
    "\n",
    "You can find out an array's shape by using its ```.shape``` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = np.random.rand(6,2)\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try creating a new array b with the same shape as a, filled with any value you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we would like to merge the two arrays, a and b, into a single matrix. Assume the columns of a and b represent the same variable, and each of them contains 6 participants. So we'd like to stick b to the bottom of a. You can do this with ```np.concatenate```. See the <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.concatenate.html\">documentation</a> to find out how to do it. Store the concatenated array in a new variable, c.\n",
    "\n",
    "Play with the ```axis``` parameter to see what different shapes you get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make another array, d, with shape (3,4). Try concatenating it with array c. Why isn't this working?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d doesn't match the dimensions of c. Let's say, we recorded our data for d in a slightly different way, using wide format. Now we'd like to change that, to make sure we can stich all the arrays together as desired. The ```.reshape``` attribute of an array can do this for us. Use ```.reshape``` on d (```d.reshape()```) in the above cell, to make it the same shape as a and b. See  the <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.ndarray.reshape.html\">documentation</a> for details. Afterwards, concatenate c and d and save them back in the variable c.\n",
    "\n",
    "Finally, we want to replace some values in our array c. You can index into the array, and replace values, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "c[2,0] = 1 #index into the array: [row,column]\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the 10th item in the first (0th) column to the day of the month in which you were born, and the 3rd item in the second (1st) column to the number of the month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'd like to do some math. Let's find out the mean and standard deviation of our two variables, using ```np.mean``` and ```np.std```. Use the argument ```axis=0``` to specify you want one mean/SD per column. ```axis=1``` will give you one mean/SD per row (maybe you want to get each participant's mean?). Leaving the axis argument unspecified will give you a single mean over the entire array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numpy has many more useful functions and applications, and if you want to get deeper into Machine Learning in python, I encourage you to dive further into <a href=\"http://www.numpy.org/\">the numpy documentation</a>. For example, Neural Networks - the current state of the art in many areas of Machine Learning today - are hugely based on matrix multiplications. Even if you are a total champ at Linear Algebra, and could implement them all by yourself, many PhDs have gone into making numpy super fast and efficient, which is why you should always choose numpy instead. Btw, even Deep Learning Frameworks such as <a href=\"https://pytorch.org/tutorials/index.html\">pytorch</a> use similar data types and functions; so understanding them is a very transferrable skill!\n",
    "\n",
    "For now, we won't need a too in-depth understanding of numpy arrays and functions. Just know that they exist, and that some of the errors you might get are due to an array being of the wrong shape or datatype. Importantly, numpy functions can be applied to pandas DataFrame/Series objects, which we will be using in this course to store and manipulate data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Pandas\n",
    "\n",
    "The two main data structures in pandas are ```pd.Series``` and ```pd.DataFrame```. Basically, each *column* in a pandas <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.html\">DataFrame</a> is a <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html\">Series</a>, which can be manipulated like a numpy array. Have a quick browse through the linked documentation to find out more about pandas Series and DataFrames. A main difference between numpy arrays and pandas DataFrames (and why I often use the latter), is that DataFrames allow for mixed DataTypes. For example, you might have a categorical variable represented as a string, and some numerical ones. Now, eventually you'll have to make all variables into numbers (most classifiers don't have that conversion inbuilt, like the models in R), and you might even want to transform your dataframe back into a numpy array *at some point*. But for manipulating and *seeing* the data, pandas is great.\n",
    "\n",
    "Let's start by making a Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make a pd.Series() object from a numpy array\n",
    "column1 = pd.Series(np.arange(3))\n",
    "column1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's make another one, using a list\n",
    "column2 = pd.Series([\"male\",\"female\",\"other\"])\n",
    "column2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the Series has not only a column with the values you gave it, but also an index, specifying the observation number/ID. You may change the index if you like, by passing it as an explicit argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set the index to something we want\n",
    "column1 = pd.Series(np.arange(3), index=['A','B','C'])\n",
    "column1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we want to change that back, so the two columns share the same index:\n",
    "column1 = column1.reset_index()[0]\n",
    "column1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```reset_index()``` resets the index to the default numbering and makes the current index a new column -- it automatically creates a dataframe. To select only the original column1, I specified [0], which was the default name given to our column1.\n",
    "\n",
    "Now we can concatenate the two columns into a dataframe, like we did with numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df= pd.concat([column1,column2],axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ooops - now we screwed up the column names. We can use the ```.columns``` attribute of the df tho change that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#set the column names of a DataFrame:\n",
    "df.columns = [\"column1\",\"column2\"]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#index into the dataframe like this:\n",
    "df[\"column2\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#you can use boolean logic to pick out specific elements:\n",
    "df[df[\"column1\"]>=1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also modify your dataframe. However, you cannot use regular indexing for this, as regular indexing only shows you a \"slice\" of the dataframe, and does not let you modify it. Use ```.loc``` for name-based indexing (using the names of the rows and columns), or ```.iloc``` for index-based indexing (using only numbers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.loc[0,\"column2\"] = \"female\"\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#change it back with .loc\n",
    "df.iloc[0,1] = \"male\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can add a new column to the dataframe, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add a new column\n",
    "df['column3'] = [1.5,24,14.5]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can create a new variable based on one or more old variables, for example by adding them together, or applying some boolean logic:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Adding two columns\n",
    "df['column4'] = df['column1'] + df['column3']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Boolean logic: see which observations have a high value in column4:\n",
    "df['column5'] = 'low'\n",
    "df.loc[df['column4']>10,\"column5\"] = \"high\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn:\n",
    "- change the value of column1 for the first participant to 7\n",
    "- make a new variable, where every value is the mean of column 3 (hint: you can use numpy functions on pandas Series)\n",
    "- can you make a new variable that has the mean of all the numeric columns for each row (hint: see <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.mean.html\">this documentation</a> for help)?\n",
    "- make a new boolean variable which is 1 for all females and 0 otherwise\n",
    "- display the final dataframe after the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Triangles Datasets\n",
    "\n",
    "Okay. Let's get started on some real data. Here's a short intro to the triangles dataset we'll be using throughout the course:\n",
    "\n",
    "A common way to test Theory of Mind abilities in neuropsychiatric conditions is the Animated Triangles test (<a href=\"https://www.sciencedirect.com/science/article/pii/S0885201400000149\">Abell et al. 2000</a>). Participants are asked to watch 8-10 videos of triangles moving across the screen and to describe then. Half of the videos contain random movements, half micro-scenarios (e.g. triangles chasing each other). The descriptions are then scored (by hand) by psychologist in terms of whether the intentional vs. random nature of the triangles is correctly inferred. The descriptions are not further analysed. Given the widespread use of the test, and the semi-controlled nature of the descriptions (all regarding relatively simple scenarios), the transcriptions are a very rich source of potential information about the cognitive and social ability of the participants. In the two datasets provided, you have data collected from people with schizophrenia, people with depression and matched controls for each group.\n",
    "\n",
    "For your convenience, the text descriptions of triangles by participants have been pre-processed into bag-of-words features. There is one row per participant, one column per word, and one column indicating the \"label\" of that participant, i.e. diagnosis. Features that occur only once in the entire dataset have been removed (to make this more manageable and reduce the number of features; since they wouldn't be predictive between training and testing). Tomorrow, you will train a classifier that categorises each participant as depressed/not depressed and schizophrenic/not schizophrenic, based on the words they use.\n",
    "\n",
    "Today, we'll do some data exploration and plotting. We'll start with the scezophrenia data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "data_path = os.path.join(os.getcwd(), '../data', 'triangles_schizophrenia.csv')\n",
    "data_s = pd.read_csv(data_path,index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```pd.read_csv``` creates a new dataframe from a csv file. Set the first column as index_col, since I wrote the index already to the csv when creating this data. See <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_csv.html\">the documentation</a> of ```pd.read_csv``` for details.\n",
    "\n",
    "Try to get an overview over the data by executing the cells below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_s.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_s.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_s.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary:\n",
    "\n",
    "- ```.head(x)``` - returns the first x instances of the dataframe (default: 5, if no x given)\n",
    "- ```.tail(x)``` - returns the last x instances of the dataframe (default: 5, if no x given)\n",
    "- ```.shape``` - the shape of the dataframe, in the form: (rows, columns)\n",
    "- ```.info()``` - returns the dataframes metadata\n",
    "- ```.describe()``` - returns a new dataframe, which contains all variable counts, means, sds, min, max and quantile information\n",
    "\n",
    "What's the shape of this dataframe? How many participants, how many variables? What types of variables are there? How many patients, how many controls (*hint: use <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.value_counts.html\">```pd.Series.value_counts()```</a> on ```data_s['label']```*)? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now load and examine the depression dataset (```../data/triangles_depression.csv```; store this in the variable ```data_d```). Answer the same questions as before, using  ```.head(x)```,```.tail(x)```, ```.shape``` , ```.info()```, ```.describe()``` and ```.value_counts()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here. (feel free to add new cells below)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matplotlib and seaborn intro\n",
    "\n",
    "This is not a data visualisation class; and anyways I am sure you are all expert plotters in R by now. If you want to keep plotting in R, be my guest; it even <a href=\"https://www.datacamp.com/community/blog/jupyter-notebook-r\"> integrates with the jupyter environment as well!</a>. Sometimes it can be handy thought to have a few python plotting skills, if you just quickly want to see what's going on in your data, without switching the environment. I also prefer the matplotlib/seaborn styles; but that's personal preference. You can learn to do <a href=\"https://matplotlib.org/resources/index.html\">everything you want and more</a>, for example how to:\n",
    "- put multiple plots next to each other and share the axes\n",
    "- change font sizes and styles\n",
    "- change all the labels\n",
    "- set different sub-tiltles for each plot and a super-title for the entire figure\n",
    "- change the size of the figure\n",
    "- save the figure (as png, pdf, ...)\n",
    "- etc.\n",
    "\n",
    "These are things I frequently look up, too. Here, we'll just cover some basics to get you started.\n",
    "\n",
    "First, let's just plot the usage of a single word, \"jeg\", by schizophrenic patients and controls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot\n",
    "f = sns.barplot(data_s[\"label\"],data_s[\"jeg\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make that a little better. We can add a title and rename the axes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#barplot\n",
    "f = sns.barplot(data_s[\"label\"],data_s[\"jeg\"])\n",
    "plt.xlabel(\"\")\n",
    "plt.xticks(np.arange(2),[\"non-schizophrenic\",\"schizophrenic\"])\n",
    "plt.ylabel(\"Mean number of occurrences\")\n",
    "plt.title(\"Occurence of the word 'jeg' in schizophrenic patients and matched controls\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try it out for yourself. Choose another word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not try using the <a href=\"https://seaborn.pydata.org/generated/seaborn.distplot.html\">sns.distplot</a> funcion to make a histogram instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Extra: If you want, figure out how to get seperate histograms for Patrients and Controls. Hint: you can use boolean logic on the dataset. If you call ```sns.distplot``` twice (on subsequent lines, using different parts of the dataset), maplotplib will automatically choose a new colour for the second histogram and overlay them in the same plot. Also try playing a bit with the parameters of ```distplot``` (check the documentation above), to see which representation you like best (e.g., try setting ```kde=False``` or ```rug=True```).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Zipf's Law\n",
    "\n",
    "Let's start by looking at Zipf's law: The law defining the distribution typical of all language data, across genres and levels of linguistic description (phonemes, graphemes, words, syntax, ...). Zipf's law should also apply to the distribution of words in our text corpus. To test this, let's \n",
    "1. Make a copy of only the features (=word, not the label), using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.drop.html\">pd.DataFrame.drop()</a> method on the data. Save this in a new dataframe.\n",
    "2. Get the total number of occurrences of each word in the new (features only) dataframe, using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.sum.html\">pd.DataFrame.sum()</a> method. This will return a pandas Series. Save it in a new variable (```total_word_counts```).\n",
    "3. Sort the ```total_word_counts``` Series in descending order (hint: use ```ascending=False```), using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.sort_values.html\">pd.Series.sort_values()</a> method.\n",
    "4. Now you can plot the values, using ```plt.bar()``` (<a href=\"https://matplotlib.org/api/_as_gen/matplotlib.pyplot.bar.html\">documentation</a>). This takes a list of scalars (x) and a height (y) for the bars. Use an ```np.arange(len(total_word_counts))``` as x -- this will just give each word its rank, i.e. the most frequent word has rank 0 and the least frequent word has the rank corresponding to the length of the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe it's a bit hard to see, since the bars are so tiny? Try displaying only the first 100 most frequent words (*hint: you can subset the Series with [:100], like a list*). Can you recognize the typical Zipfian pattern?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code here. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Variable relationships\n",
    "\n",
    "Right. So far, our observations have been pretty general: We know what kind of data we have, how many observations and features, and how those features are generally distributed. We have also compared the use of specific words in the two groups. But what about the *relationships* between words?\n",
    "\n",
    "In our Naive Bayes model (more on that in Lab2), we make the assumption that features are conditionally independent of one another. Let's get a first impression about how realistic that assumption is. Let's start (arbitrarily) with the first 5 words in our dataset, and see how correlated they are. <a href=\"https://seaborn.pydata.org/generated/seaborn.pairplot.html\">```sns.pairplot```</a> conveniently makes plots for all combinations of these 10 variables. Include the line of best fit (for estimating the correlation) by specifying the argument ```kind='reg'```.\n",
    "\n",
    "*Hint: ```data_s.iloc[:,:5]``` will give you the same dataframe, but using only the fist 5 veriables.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are these variables uncorrelated? Which ones are, which ones aren't? Bonus: Do you think the ones that are correlated could be <b>conditionally</b> independent, given the class (patient/control)? Why/why not? (You don't have to test this, just use your intuition and understanding of the concept and of language.)\n",
    "\n",
    "Optionally: pick a few other words (maybe that you think could be correlated) and examine their relationship, using ```sns.pairplot``` and <a href=\"https://seaborn.pydata.org/generated/seaborn.regplot.html\">```sns.regplot```</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Code here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Exploring distributions and relationships in depressed participants [A]\n",
    "\n",
    "If you have time, you may want to explore the depression dataset in a similar way as we have done before. Think about Zipf's law, variable relationships. Maybe compare a few particular words that you think could be used more or less frequently by depressed patients, by making a barplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
