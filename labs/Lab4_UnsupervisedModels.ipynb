{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Unsupervised Learning\n",
    "\n",
    "In this lab, we'll be clustering our participants using unsupervised learning, to discover the sturcture in our dataset.\n",
    "\n",
    "First, let's start with some imports, as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing and exploring the Data\n",
    "\n",
    "This time, we'll start with the depression dataset from our trinagles data. Load ```'triangles_depression.csv'``` from the ```../data``` directory as before. Use ```.shape``` and ```.head()``` to see if the data looks alright, and to check the number of features and participants:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how our classes are distributed. Use <a href=\"https://seaborn.pydata.org/generated/seaborn.countplot.html\">```sns.countplot()```</a> to generate a bar plot showing the distribution of ```data_d['label']```. What groups are there? How many are in each group (*hint: use ```.value_counts()```*)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, like in lab 2, split the data into a feature matrix ```X``` and a target label vector ```y```. Use the ```.shape``` attribute to check the dimensionality. Why do we not need to do any train-test splitting in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. k-means clustering\n",
    "\n",
    "Now, we are ready to use the k-means clustering algorithm. Use the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">```KMeans()```</a> class for this. Let's first assume we know that there are 3 distinct groups of people in our dataset - patients with a first episode of depression, chronically depressed patients and controls. So we set the number of clusters to 3. To make your experiment repeatable, set the ```random_state = 12``` (*think about: what would happen if you didn't set a random state?*). Make sure you only fit KMeans to X, not y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to evaluate how good our clustering is, by comparing the amount of overlap between the clusters found with unsupervised learning and our original labels. sklearns <a href=\"http://scikit-learn.org/0.17/modules/generated/sklearn.metrics.adjusted_rand_score.html\">```adjusted_rand_score()```</a> function will do this for us. Read its documentation and try to understand what it does; then call it on ```y``` and the ```.labels_``` attribute of your KMeans() object (that you fit above). How good is the result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualise what's going on. For each cluster, plot its distribution of classes (*hint: you may use ```sns.countplot``` again, or something else if you like*). What do you observe? How do classes and clusters match up? Can you think of any reasons for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*\n",
    "\n",
    "The distrubtion of classes is the same in all clusters - pretty random clustering!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Again like in lab 2, let's do some feature engineering. By now, you should have encountered some techniques to select features or reduce feature dimensionality. Try at least one thing to modify your features, in order to enhance the performance of the k-means clustering algorithm, e.g.:\n",
    "\n",
    "- remove stopwords\n",
    "- transform features into a lower-dimensional space, using PCA (see lab 3, XX?) [A]\n",
    "- select features with high information gain, using a decision tree (see lab 2, 9. - just do this on the full dataset for now, although this is technically cheating. Really, you should be using a held-out set to test the quality of the feature selection) [A]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, fit KMeans (3 clusters, ```random_state = 12```) to this data and print the adjusted rand index for this clustering. Has the clustering improved? Take some notes and/or make a plot if you like, to help you udnerstand what's going on inside those functions/objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Finding k\n",
    "\n",
    "Let's assume now we don't already *know* how many classes there are, but we want to *discover* an optimal number of clusters. For example, we might be unsure about whether to consider first episode and chronic patients as part of the same group, or two different groups. We might also not know what kinds of different people we have at all, or we might want to find out if there are different stages of depression that manifest themselves more gradually - maybe there are 4, or 5 different subgroups?\n",
    "\n",
    "We'll try to discover an \"optimal\" number of k by looking at the scree plot (take \"optimal\" with a grain of salt here - this is a very data-driven approach that you should not take more seriously than your top-down knowledge on a specific subject!).\n",
    "\n",
    "Run the following steps:\n",
    "1. Create a range of values for k to explore, from 1 to 10\n",
    "2. Create an array to store the sum squared distances with each k (*hint: this is also called \"intertia\"*)\n",
    "3. Iterate over your k-values, fit a KMeans object to X, using each k (set the ```random_state = 0```), and store the intertia\n",
    "4. Make a plot (e.g., using ```sns.pointplot```) of k-values versus intertia\n",
    "5. Find the \"ellbow\"\n",
    "\n",
    "What do you observe? What is the \"optimal\" number of clusters? Does this match your expectation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GMM-clustering\n",
    "\n",
    "Let's try some GMM-clustering now. Before you start, think about how k-means is similar to clustering with GMMs. What's the difference between the two approaches? Do you expect the GMM clustering to work better, or worse, than k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's make a Gaussian Mixture Model. Use the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.mixture.GaussianMixture.html\">```GaussianMixture()```</a> class. Again, let's start with 3 components (```random_state=1```). Fit the model and predict a cluster for each data point in X (*why are we using the training data here, again?*). Then print the adjusted rand score between y and the predicted clusters. Do you see any improvements?\n",
    "\n",
    "Again, plot the predicted clusters versus true labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Big(ger) data! [A]\n",
    "\n",
    "The dataset we've been working with is fairly small - in fact, too small for unsupervised learning. Further, the task (triangles) is pretty constrained, and may not reveal all the variety in language use by different people. A bigger dataset, with more variety, is the <a href=\"https://www.kaggle.com/c/fake-news/data\">Kaggle Fake News challenge dataset</a>. It originally contains ca. 150k news articles. I've pre-processed the first ca. 5k articles for you, in much the same way as our triangles data. The labels are \"0 = reliable\" and \"1 = unreliable\". \n",
    "\n",
    "Load the dataframe from ```../data/fakenews.csv``` into a new variable called ```newsdata```. Examine the shape and head of the dataframe to see what it looks like. How many observations are there? How many features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the clustering techniques you have learned about to detect groups of articles in this dataset. Choose either k-means or GMMs to cluster the data. What number of clusters/mixtures do you choose? Do clusters and labels line up well? How many subgroups of articles do you find, using a scree test? Use plots and evaluation metrics and reflect on your findings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Clustering word vectors [A]\n",
    "\n",
    "Maybe it is a bit much to ask of our model, to discover groups of patients and controls or news articles all on its own, with no supervision and only a bag of words as features. Let's try clustering something that seems intuitively more \"groupable\": Let's try and discover different classes/categories of words, using our word vectors from lab 3.\n",
    "\n",
    "First, load the word vector dataset from lab 3 (using the ```load_vectors()``` function below on ```../data/word_vectors.vec```). Check the head and dimensionality of the resulting dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = tokens[1:]\n",
    "    data = pd.DataFrame.from_dict(data,orient=\"index\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do some word clustering.\n",
    "1. Prepare a feature matrix X from the dataframe above\n",
    "2. Define a range of k-values to explore\n",
    "3. Fit KMeans to X, using these k-values. Store the resulting mean squared error for each k. (Use ```random_state=0```)\n",
    "4. Make a scree plot and determine the optimal k\n",
    "5. Fit k-means again with the optimal k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's explore one of the clusters. Let's just use cluster one - have a look at the words it contains:\n",
    "1. Get the label of each datapoint from your KMeans object (check out the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\">documentation</a> to see which attribute to use). Add this as a new column to your dataframe.\n",
    "2. Find the words where label==1 and display them. (*Hint: Pick out the matching rows using boolean logic. You can use .index to see the word associated with a particular row in the dataframe.*)\n",
    "\n",
    "Do you see any pattern? (Try a few other clusters as well, and determine whether you think there is any pattern in this clustering.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Findings\n",
    "\n",
    "Think about your findings. What have they taught you?\n",
    "- What are the advantages of unsupervised methods? \n",
    "- Which approach worked better - k-means or GMM?\n",
    "- What are the challenges with unsupervised learning?\n",
    "- Were the assumptions met?\n",
    "- Have you learned anything about depression/schizophrenia?\n",
    "- Do you think people with depression/schizophrenia can be clustered using only words from triangles descriptions, generally? What do you think we would need to change to make better predictions?\n",
    "- Can unsupervised learning help you discover anything new about your data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take some notes here:*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
