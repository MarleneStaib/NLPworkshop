{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Vector Semantics \n",
    "\n",
    "In this lab, we'll be using some pre-trained word vectors to investigate phenomena such as word similarity, word analogies and semantic bias. \n",
    "\n",
    "Let's start by importing some stuff, as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import io, os, itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 1. Word similarities\n",
    "\n",
    "The word vectors we are using are pre-trained \"fastText\" embeddings by Mikolov et al. (2017): <a href=\"https://arxiv.org/abs/1712.09405\">Advances in Pre-Training Distributed Word Representations</a>. The full set consists of 1 million word embeddings, trained on a corpus of 16 billion word tokens (Wikipedia 2017, UMBC webbase corpus and statmt.org news dataset). It can be found <a href=\"https://fasttext.cc/docs/en/english-vectors.html\">here</a>. <a href=\"https://fasttext.cc/\">fastText</a> also includes a library for building classifiers - feel free to have a look at it in your own time.\n",
    "\n",
    "The size of the corpus and vocabulary used can give you an impression of the kinds of resources needed to train good vector representations, which is why we won't do it ourselves in this lab. The implementation itself is not beyond your level, and if you have too much time and a massive server at you disposal, feel free to try and implement some of the collocation vector methods we discussed, such as TF-IDF or PPMI.\n",
    "\n",
    "These embeddings we're using here were obtained with a Neural Network. More on Neural Networks will follow later in this course. The basic idea is that it is a classifier trained to predict a word from its sourrounding context words. We then use the some of the *parameters* of that classifier as our word vectors. Word vectors from Neural Networks are usually called *embeddings*. Conceptually, this is very similar to collocation word vectors: *\"know a word by the company it keeps\"*. Mathematically, it is a bit different, because context words are never counted, and the Neural Net uses a different way of trial and error to find an optimal solution. It is also convenient because the result is a dense, low-dimensional word vector, instead of the sparse, high-dimensional collocation vectors we would get. This means that we can represent each word as a vector of 300 (float) numbers, instead of many thousands of counts (most of which are 0 anyways). \n",
    "\n",
    "The following code loads the word vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#function from https://fasttext.cc/docs/en/english-vectors.html to read the word vectors from file\n",
    "def load_vectors(fname):\n",
    "    fin = io.open(fname, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "    n, d = map(int, fin.readline().split())\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.rstrip().split(' ')\n",
    "        data[tokens[0]] = np.array(tokens[1:],dtype=float).reshape((1,-1))\n",
    "    return data\n",
    "\n",
    "#load the word vectors\n",
    "data_path = os.path.join(os.getcwd(), '../data', 'word_vectors.vec') # just using the first 10k fastText words!\n",
    "data = load_vectors(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, I've selected the first 10k word embeddings for you to play with (the full 1M would crash my computer). Use the following function to check whether a word is in the selection or not (try it out on a few words of your choice):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_vector_exists(word):\n",
    "    exist_status = \"\" if word in data.keys() else \" not\"\n",
    "    return 'The word \"{}\" is{} in the dataset.'.format(word, exist_status)\n",
    "\n",
    "#try calling this function on some words\n",
    "word_vector_exists(\"dog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start looking at some similarities. We'll use the cosine similarity to test similarity between different pairs of words.\n",
    "\n",
    "Below, write a function ```get_word_similarity()``` that\n",
    "1. Takes 2 words as input\n",
    "2. Checks if the words are both in the dataset\n",
    "3. If one or both of the words are not in the dataset, prints an error message informing the user which word(s) was/were not found\n",
    "4. If both words are in the dataset, retrieves their word vectors and computes their cosine similarity (just use <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.cosine_similarity.html\">scikit-learn's implementation</a>\n",
    "5. Returns the cosine similarity.\n",
    "\n",
    "Call the function on the following word pairs to test your function:\n",
    "- \"cat\", \"dog\"\n",
    "- \"cat\", \"cat\"\n",
    "- \"cat\", \"marsupial\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Semantic similarity\n",
    "\n",
    "Let's test some semantic similarities. Below, we have two lists of nouns from different categories:\n",
    "- furniture\n",
    "- furry animals\n",
    "Let's test the hypothesis that same-category word pairs are more similar to one another than different-category word pairs.\n",
    "\n",
    "Execute the code below to create 3 lists of word pairs:\n",
    "1. furniture - furniture\n",
    "2. animal - animal\n",
    "3. furniture - animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#word lists\n",
    "animals = [\"cat\", \"bear\", \"dog\", \"mouse\", \"sheep\"]\n",
    "furniture = [\"table\", \"chair\", \"desk\", \"bed\", \"board\"]\n",
    "\n",
    "#pepare the word pairs\n",
    "animal_pairs = [a for a in itertools.combinations(animals,2)]\n",
    "furniture_pairs = [a for a in itertools.combinations(furniture,2)]\n",
    "animal_furniture_pairs = [(a, b) for a in animals for b in furniture]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, to see if our similarity scoring works as expected:\n",
    "1. Get a similarity score for each word pair, using your scoring function and the word lists above. Store it in a list/array\\*.\n",
    "2. Plot the similarity scores, using a plot of your choice. *Hint: The plot should be somehow grouped by categorgy, so you may have to create another array with the category label (animal, furniture, animal-furniture) for each word pair*\n",
    "\n",
    "Is the result what you expected?\n",
    "____________\n",
    "\\*For the array, you have to define the size in the beginning, which is ```len(animal_pairs + furniture_pairs + animal_furniture_pairs)```. If you are using a list, you can append scores, but be careful to turn the list into an ```np.array()``` before trying to plot the values!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Syntactic similarity\n",
    "\n",
    "Now test whether adjectives are more similar to other adjectives, compared to adjective-noun pairs.\n",
    "1. Create a list of 5 adjectives and another list of 5 nouns that are all in the data\n",
    "2. Find the possible combinations, like I did above\n",
    "3. Score the similarity of each pairing\n",
    "4. Plot the similarities of pairs with the same or different part-of-speech\n",
    "\n",
    "Are the results as expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Find closest\n",
    "\n",
    "So far, we've been comparing similarities of chosen pairs of words. Let's now see if we can automatically find the most similar word to any given input word, using our word vectors. \n",
    "\n",
    "Write a function ```find_closest()``` that:\n",
    "\n",
    "1. Takes 1 word as input\n",
    "2. Checks if that word is in the data. If not, prints a warning for the user and returns immediately\n",
    "3. Iterates over all the words in the data, computes the similarity between the input word and every word, and finds the one with the highest cosine similarity.\\*\n",
    "4. Returns the most similar word.\n",
    "\n",
    "Test your function on a few words of your choice.\n",
    "_____\n",
    "\\* A few hints for step 3:\n",
    "- Use some variables (outside the loop) to keep track of which word is currently the most similar, and which score it has\n",
    "- Make sure the input word itself doesn't get chosen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 2. Analogies\n",
    "\n",
    "Let's do some maths with words now! We'll write a function to solve the following analogies:\n",
    "\"King is to man like ____ is to woman\". Formally, we can try to get the \"blank\" word by subtracting \"man\" from \"king\" and adding \"woman\":\n",
    "\n",
    "____ = King - man + woman\n",
    "\n",
    "Write a function that:\n",
    "1. Takes 3 words as input\n",
    "2. Checks if all 3 words are in the data; if not, print a warning for the user and return\n",
    "3. Retrieves the word vectors for the 3 words and computes an ```analogy_vector```: word1 - word2 + word3. This should result in a new word vector\n",
    "4. Finds the word in the data that has the most similar word vector to that ```analogy_vector```. *Hint: You may have to make a modified vecion of your* ```find_closest``` *function, which takes as input a vector instead of a word, and returns the closest word.*\n",
    "5. Make sure the word that is returned is <b>not</b> one of the original 3 input words! *Hint: add that condition to your new* ```find_closest2``` *function*.\n",
    "\n",
    "Try finding the following analogies with your function:\n",
    "- King - man = ____ - woman\n",
    "- France - Paris = ____ - Rome\n",
    "- Actor - man = ____ - woman\n",
    "\n",
    "Try a few more analogies on your own!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Good words and bad words\n",
    "\n",
    "You might have come across sentiment analysis, and <a href=\"http://sentiwordnet.isti.cnr.it/\">the kinds of word lists with human sentiment ratings</a> that help you decide how positive/negative each word is.\n",
    "\n",
    "Let's now make automatic \"sentiment\" ratings: Assuming a positive word is more close to positive words than negative words, we can use the similarity score between an input word and a list of positive/negative words as our \"sentiment score\".\n",
    "\n",
    "Below, write a function ```sentiment_score``` that:\n",
    "1. Takes 1 word as input\n",
    "2. Checks if that word is in the data. If not, prints a warning for the user and returns immediately\n",
    "3. Defines 2 lists of minimum 5 words each: ```positive_words``` and ```negative_words``` (*use 'obviously' good/bad words such as \"good\", \"bad\", \"positive\", \"negative\", ... Make sure they are all in the data!*)\n",
    "4. Calculates a similarity scores between the word and a.) each positive word b.) each negative word (keep the positive and negative scores in seperate arrays)\n",
    "5. Calculates the average ```positive_similarity``` and ```negative_similarity```\n",
    "6. Calculates a sentiment score as: ```positive_similarity - negative_similarity```\n",
    "\n",
    "Try a few words of your choice. Is the result expected? (Words we expect to be negative should get a score below 0, words we expect to be positive should get a score above 0!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Visualising word embeddings/PCA [A]\n",
    "\n",
    "So far, we've gained an intuition about the power of word embeddings, using similarity scores and analogies. But it's a bit hard to imagine/visualise these word vectors, who live in a 300-dimensional space (let alone 100k-dimensional or higher, as might be the case for sparse collocation vectors!). \n",
    "\n",
    "Let's reduce the dimensionality of these word embeddings to 2, and plot them in a 2-d space, to get an impression of what they look like. Dimensionality reduction is also useful for turning sparse word vectors into dense, lower-dimensional ones. It can also help improve the performance of certain classifiers, if we transform our input features to a lower-dimensional space.\n",
    "\n",
    "Here, we'll use Principal Component Analysis (PCA) to transform out vector embeddings into 2-D representations. We won't have time to go into the theory behind PCA in this course, but if you are interested and have time, <a href=\"https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues\">this Stackoverflow question and its top answer</a> is the best intro to PCA I have read so far. Feel free to read to whichever level (grandmother, mother, spouse, daughter) you feel comfortable with ;) It's a technique well worth knowing about in Machine Learning, so if you are interested and have some spare time in the end of this lab, I encourage you to read up about it (and maybe google for some additional materials).\n",
    "\n",
    "The basic idea is to find to dimensions of most variance, turn the parameter space to line up with those dimensions, an then flatten everything like a pancake. Don't worry if this doesn't make sense to you right now - luckily, sklearn will do the rest of the maths for us.\n",
    "\n",
    "To prepare the data, you need to merge all the word vectors into a single numpy array called X (with shape (10000,300)). I've provided some code below to help you do this (this might take a minute or so to run):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the data\n",
    "X = np.empty((0,300)) #initialise an empty array with no values, to concatenate stuff to in the loop below\n",
    "\n",
    "for word in data.keys():\n",
    "    X = np.concatenate((X,data[word]))\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate the <a href=\"http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\">```PCA```</a> class with ```n_components=2```. ```.fit()``` PCA to ```X```. \n",
    "\n",
    "Finally, pick a few words to plot (e.g., use the animals and furniture lists from above). Look up and transform each word vector in the list, using the PCA object's ```.transform()``` method. Store the transformed vectors in a new array (use one array per category, e.g., ```X_animals``` and ```X_furniture```). Now you can plot the two word vector dimensions ```X_animals[:,0]``` and ```X_animals[:,1]```, using ```sns.scatterplot()```. *Hint: If you call* ```sns.scatterplot()``` *twice in a row with different input arrays, and then use* ```plt.show()```, *matplotlib will automatically plot the two data arrays in the same figure, using different colours.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Findings\n",
    "In this lab, we've just been playing with pre-trained vectors, so our findings about real linguistic and conginive phenomena may be limited. Nevertheless, reflect on the kinds of things you have explored in this lab, and what they have taught you about words and word embeddings:\n",
    "\n",
    "1. Does the distributional hypothesis seem to be correct? Do word embeddings reflect meaningful connections/similarities between words?\n",
    "2. Are there any biases reflected in these word embeddings? Can you think of a scenario were the relationships between word embeddings would be problematic (e.g., from an ethical perspective)?\n",
    "3. Can you use word vectors for sentiment classification? Could you think of any other classification task where you might apply a similar method?\n",
    "4. How could you use word embeddings to improve our depression/schizophrenia classifier (from lab 2)? What would be the features? Is there a new way of representing a sentence?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Take notes here:*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp_workshop"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
